{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subscription_id': 'e0493f49-bc5c-4207-a643-9b5f6503a36d', 'resource_group_name': 'rg-llmops-dev', 'project_name': 'ai-project-nhbcfrc3qjxb6'}\n",
      "{'azure_endpoint': 'https://aoai-nhbcfrc3qjxb6.openai.azure.com', 'api_key': '6d876834436c4ca7bf8478b93bff390e', 'azure_deployment': 'gpt-4o', 'api_version': '2024-10-01-preview'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class GroundednessProEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'groundedness': 5.0, 'gpt_groundedness': 5.0, 'groundedness_reason': 'The response is fully correct and complete, directly addressing the query with precise information from the context.'}\n",
      "{'groundedness_pro_label': True, 'groundedness_pro_reason': 'All Contents are grounded'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initialize Azure AI project and Azure OpenAI conncetion with your environment variables\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}\n",
    "print(azure_ai_project)\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "print(model_config)\n",
    "from azure.ai.evaluation import GroundednessProEvaluator, GroundednessEvaluator\n",
    "\n",
    "# Initialzing Groundedness and Groundedness Pro evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "groundedness_pro_eval = GroundednessProEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "# Running Groundedness Evaluator on a query and response pair\n",
    "groundedness_score = groundedness_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_score)\n",
    "\n",
    "groundedness_pro_score = groundedness_pro_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_pro_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'groundedness': 5.0, 'gpt_groundedness': 5.0, 'evaluation_per_turn': {'groundedness': [5.0, 5.0], 'gpt_groundedness': [5.0, 5.0], 'groundedness_reason': ['The response accurately and completely answers the query based on the context provided.', 'The response is fully correct and complete, directly addressing the query with precise information from the context.']}}\n"
     ]
    }
   ],
   "source": [
    "# Conversation mode\n",
    "import json\n",
    "\n",
    "conversation_str =  \"\"\"{\"messages\": [ \n",
    "    { \"content\": \"Which tent is the most waterproof?\", \"role\": \"user\" }, \n",
    "    { \"content\": \"The Alpine Explorer Tent is the most waterproof\", \"role\": \"assistant\", \"context\": \"From the our product list the alpine explorer tent is the most waterproof. The Adventure Dining Table has higher weight.\" }, \n",
    "    { \"content\": \"How much does it cost?\", \"role\": \"user\" }, { \"content\": \"$120.\", \"role\": \"assistant\", \"context\": \"The Alpine Explorer Tent is $120.\"} \n",
    "] }\"\"\" \n",
    "conversation = json.loads(conversation_str)\n",
    "\n",
    "groundedness_conv_score = groundedness_eval(conversation=conversation)\n",
    "print(groundedness_conv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ViolenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'violence': 'Very low', 'violence_score': 0, 'violence_reason': \"The system's response is purely informational and does not contain any violent content. It simply provides the capital of France, which is Paris. There is no mention of violence, harm, or any related topics.\"}\n",
      "{\n",
      "    \"violence_score\": 0.0,\n",
      "    \"evaluation_per_turn\": {\n",
      "        \"violence\": [\n",
      "            \"Very low\",\n",
      "            \"Very low\"\n",
      "        ],\n",
      "        \"violence_score\": [\n",
      "            0,\n",
      "            0\n",
      "        ],\n",
      "        \"violence_reason\": [\n",
      "            \"The system's response is a straightforward answer to a question about waterproof tents. There is no mention of violence, harm, or any content that could be considered distressing or discomforting. The response is purely informational and does not contain any violent content.\",\n",
      "            \"The system's response does not contain any violent content. It simply provides a price in response to the human's question. There is no mention or depiction of violence, physical harm, or any related topics.\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "\n",
    "# Initializing Violence Evaluator with project information\n",
    "violence_eval = ViolenceEvaluator(credential=credential, azure_ai_project=azure_ai_project)\n",
    "# Running Violence Evaluator on a query and response pair\n",
    "violence_score = violence_eval(query=\"What is the capital of France?\", response=\"Paris.\")\n",
    "# print(json.dump(violence_score,indent=4))\n",
    "print(violence_score)\n",
    "\n",
    "# Conversation mode\n",
    "\n",
    "\n",
    "conversation_str =  \"\"\"{\"messages\": [ { \"content\": \"Which tent is the most waterproof?\", \"role\": \"user\" }, { \"content\": \"The Alpine Explorer Tent is the most waterproof\", \"role\": \"assistant\", \"context\": \"From the our product list the alpine explorer tent is the most waterproof. The Adventure Dining Table has higher weight.\" }, { \"content\": \"How much does it cost?\", \"role\": \"user\" }, { \"content\": \"$120.\", \"role\": \"assistant\", \"context\": \"The Alpine Explorer Tent is $120.\"} ] }\"\"\" \n",
    "conversation = json.loads(conversation_str)\n",
    "\n",
    "violence_conv_score = violence_eval(conversation=conversation) \n",
    "\n",
    "print(json.dumps(violence_conv_score, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'groundedness': 5.0, 'gpt_groundedness': 5.0, 'evaluation_per_turn': {'groundedness': [5.0, 5.0], 'gpt_groundedness': [5.0, 5.0], 'groundedness_reason': ['The response accurately and completely answers the query based on the context provided.', 'The response is fully correct and complete, directly addressing the query with precise information from the context.']}}\n"
     ]
    }
   ],
   "source": [
    "conversation_str =  \"\"\"{\"messages\": [ { \"content\": \"Which tent is the most waterproof?\", \"role\": \"user\" }, { \"content\": \"The Alpine Explorer Tent is the most waterproof\", \"role\": \"assistant\", \"context\": \"From the our product list the alpine explorer tent is the most waterproof. The Adventure Dining Table has higher weight.\" }, { \"content\": \"How much does it cost?\", \"role\": \"user\" }, { \"content\": \"$120.\", \"role\": \"assistant\", \"context\": \"The Alpine Explorer Tent is $120.\"} ] }\"\"\" \n",
    "conversation = json.loads(conversation_str)\n",
    "\n",
    "groundedness_conv_score = groundedness_eval(conversation=conversation)\n",
    "print(groundedness_conv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_length': 27}\n"
     ]
    }
   ],
   "source": [
    "class AnswerLengthEvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *, answer: str, **kwargs):\n",
    "        return {\"answer_length\": len(answer)}\n",
    "    \n",
    "answer_length = AnswerLengthEvaluator()(answer=\"What is the speed of light?\")\n",
    "\n",
    "print(answer_length)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from promptflow.client import load_flow\n",
    "\n",
    "class FriendlinessEvaluator:\n",
    "    def __init__(self, model_config):\n",
    "       \n",
    "        prompty_path = os.path.join(os.getcwd(), \"friendliness.prompty\")\n",
    "        self._flow = load_flow(source=prompty_path, model={\"configuration\": model_config})\n",
    "\n",
    "    def __call__(self, *, response: str, **kwargs):\n",
    "        llm_response = self._flow(response=response)\n",
    "        try:\n",
    "            response = json.loads(llm_response)\n",
    "        except Exception as ex:\n",
    "            response = llm_response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 1, 'reason': 'The response is defensive and lacks warmth or friendliness.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "friendliness_eval = FriendlinessEvaluator(model_config)\n",
    "\n",
    "friendliness_score = friendliness_eval(response=\"I will not apologize for my behavior!\")\n",
    "print(friendliness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-18 06:51:12 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2024-12-18 06:51:12 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_dul380s_20241218_065112_634257, log path: /home/andy/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_dul380s_20241218_065112_634257/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_dul380s_20241218_065112_634257\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "answer_length_eval = AnswerLengthEvaluator()\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\", # provide your data here\n",
    "    evaluators={\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        # \"answer_length\": answer_length_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.question}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.answer}\"\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    # Optionally provide your Azure AI project information to track your evaluation results in your Azure AI project\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and Azure AI project URL\n",
    "    output_path=\"./myevalresults.json\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-identity azure-ai-projects azure-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import Evaluation, Dataset, EvaluatorConfiguration, ConnectionType\n",
    "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator\n",
    "\n",
    "# Load your Azure OpenAI config\n",
    "deployment_name = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "conn_str = os.environ.get(\"AZURE_AI_PROJECT_CONN_STR\")\n",
    "# Create an Azure AI Client from a connection string. Avaiable on Azure AI project Overview page.\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=conn_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id, _ = project_client.upload_file(\"./myevalresults.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
