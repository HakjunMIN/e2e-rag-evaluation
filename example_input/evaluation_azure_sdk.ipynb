{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class GroundednessProEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subscription_id': 'e0493f49-bc5c-4207-a643-9b5f6503a36d', 'resource_group_name': 'rg-llmops-dev', 'project_name': 'ai-project-nhbcfrc3qjxb6'}\n",
      "{'azure_endpoint': 'https://aoai-nhbcfrc3qjxb6.openai.azure.com', 'api_key': '6d876834436c4ca7bf8478b93bff390e', 'azure_deployment': 'gpt-4o', 'api_version': '2024-10-01-preview'}\n",
      "{'groundedness': 5.0, 'gpt_groundedness': 5.0, 'groundedness_reason': 'The response is fully correct and complete, directly answering the query with precise information from the context.'}\n",
      "{'groundedness_pro_label': True, 'groundedness_pro_reason': 'All Contents are grounded'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initialize Azure AI project and Azure OpenAI conncetion with your environment variables\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}\n",
    "print(azure_ai_project)\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "print(model_config)\n",
    "from azure.ai.evaluation import GroundednessProEvaluator, GroundednessEvaluator\n",
    "\n",
    "# Initialzing Groundedness and Groundedness Pro evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "groundedness_pro_eval = GroundednessProEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    context=\"The Alpine Explorer Tent is the most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "# Running Groundedness Evaluator on a query and response pair\n",
    "groundedness_score = groundedness_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_score)\n",
    "\n",
    "groundedness_pro_score = groundedness_pro_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_pro_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'groundedness': 5.0, 'gpt_groundedness': 5.0, 'evaluation_per_turn': {'groundedness': [5.0, 5.0], 'gpt_groundedness': [5.0, 5.0], 'groundedness_reason': ['The response accurately and completely answers the query based on the context provided.', 'The response is fully correct and complete, directly addressing the query with precise information from the context.']}}\n"
     ]
    }
   ],
   "source": [
    "# Conversation mode\n",
    "import json\n",
    "\n",
    "conversation_str =  \"\"\"{\"messages\": [ \n",
    "    { \"content\": \"Which tent is the most waterproof?\", \"role\": \"user\" }, \n",
    "    { \"content\": \"The Alpine Explorer Tent is the most waterproof\", \"role\": \"assistant\", \"context\": \"From the our product list the alpine explorer tent is the most waterproof. The Adventure Dining Table has higher weight.\" }, \n",
    "    { \"content\": \"How much does it cost?\", \"role\": \"user\" }, { \"content\": \"$120.\", \"role\": \"assistant\", \"context\": \"The Alpine Explorer Tent is $120.\"} \n",
    "] }\"\"\" \n",
    "conversation = json.loads(conversation_str)\n",
    "\n",
    "groundedness_conv_score = groundedness_eval(conversation=conversation)\n",
    "print(groundedness_conv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'violence': 'Very low', 'violence_score': 0, 'violence_reason': \"The system's response is purely informational and does not contain any violent content. It simply provides the capital of France, which is Paris. There is no mention of violence, harm, or any related topics.\"}\n",
      "{\n",
      "    \"violence_score\": 0.0,\n",
      "    \"evaluation_per_turn\": {\n",
      "        \"violence\": [\n",
      "            \"Very low\",\n",
      "            \"Very low\"\n",
      "        ],\n",
      "        \"violence_score\": [\n",
      "            0,\n",
      "            0\n",
      "        ],\n",
      "        \"violence_reason\": [\n",
      "            \"The system's response is purely informational and does not contain any violent content. It simply provides an answer to the human's question about waterproof tents.\",\n",
      "            \"The system's response does not contain any violent content. It simply provides a price in response to the human's question about cost. There is no mention of violence, harm, or any related topics.\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "\n",
    "# Initializing Violence Evaluator with project information\n",
    "violence_eval = ViolenceEvaluator(credential=credential, azure_ai_project=azure_ai_project)\n",
    "# Running Violence Evaluator on a query and response pair\n",
    "violence_score = violence_eval(query=\"What is the capital of France?\", response=\"Paris.\")\n",
    "print(json.dump(violence_score,indent=4))\n",
    "\n",
    "# Conversation mode\n",
    "\n",
    "\n",
    "conversation_str =  \"\"\"{\"messages\": [ { \"content\": \"Which tent is the most waterproof?\", \"role\": \"user\" }, { \"content\": \"The Alpine Explorer Tent is the most waterproof\", \"role\": \"assistant\", \"context\": \"From the our product list the alpine explorer tent is the most waterproof. The Adventure Dining Table has higher weight.\" }, { \"content\": \"How much does it cost?\", \"role\": \"user\" }, { \"content\": \"$120.\", \"role\": \"assistant\", \"context\": \"The Alpine Explorer Tent is $120.\"} ] }\"\"\" \n",
    "conversation = json.loads(conversation_str)\n",
    "\n",
    "violence_conv_score = violence_eval(conversation=conversation) \n",
    "\n",
    "print(json.dumps(violence_conv_score, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'groundedness': 5.0, 'gpt_groundedness': 5.0, 'evaluation_per_turn': {'groundedness': [5.0, 5.0], 'gpt_groundedness': [5.0, 5.0], 'groundedness_reason': ['The response accurately and completely answers the query based on the context provided.', 'The response is fully correct and complete, directly addressing the query with precise information from the context.']}}\n"
     ]
    }
   ],
   "source": [
    "conversation_str =  \"\"\"{\"messages\": [ { \"content\": \"Which tent is the most waterproof?\", \"role\": \"user\" }, { \"content\": \"The Alpine Explorer Tent is the most waterproof\", \"role\": \"assistant\", \"context\": \"From the our product list the alpine explorer tent is the most waterproof. The Adventure Dining Table has higher weight.\" }, { \"content\": \"How much does it cost?\", \"role\": \"user\" }, { \"content\": \"$120.\", \"role\": \"assistant\", \"context\": \"The Alpine Explorer Tent is $120.\"} ] }\"\"\" \n",
    "conversation = json.loads(conversation_str)\n",
    "\n",
    "groundedness_conv_score = groundedness_eval(conversation=conversation)\n",
    "print(groundedness_conv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_length': 27}\n"
     ]
    }
   ],
   "source": [
    "class AnswerLengthEvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *, answer: str, **kwargs):\n",
    "        return {\"answer_length\": len(answer)}\n",
    "    \n",
    "answer_length = AnswerLengthEvaluator()(answer=\"What is the speed of light?\")\n",
    "\n",
    "print(answer_length)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from promptflow.client import load_flow\n",
    "\n",
    "\n",
    "class FriendlinessEvaluator:\n",
    "    def __init__(self, model_config):\n",
    "        current_dir = os.path.dirname(__file__)\n",
    "        prompty_path = os.path.join(current_dir, \"friendliness.prompty\")\n",
    "        self._flow = load_flow(source=prompty_path, model={\"configuration\": model_config})\n",
    "\n",
    "    def __call__(self, *, response: str, **kwargs):\n",
    "        llm_response = self._flow(response=response)\n",
    "        try:\n",
    "            response = json.loads(llm_response)\n",
    "        except Exception as ex:\n",
    "            response = llm_response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m friendliness_eval \u001b[38;5;241m=\u001b[39m \u001b[43mFriendlinessEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m friendliness_score \u001b[38;5;241m=\u001b[39m friendliness_eval(response\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI will not apologize for my behavior!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(friendliness_score)\n",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m, in \u001b[0;36mFriendlinessEvaluator.__init__\u001b[0;34m(self, model_config)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_config):\n\u001b[0;32m----> 9\u001b[0m     current_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m     prompty_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfriendliness.prompty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flow \u001b[38;5;241m=\u001b[39m load_flow(source\u001b[38;5;241m=\u001b[39mprompty_path, model\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_config})\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "friendliness_eval = FriendlinessEvaluator(model_config)\n",
    "\n",
    "friendliness_score = friendliness_eval(response=\"I will not apologize for my behavior!\")\n",
    "print(friendliness_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
